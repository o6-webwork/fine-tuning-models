{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "[NEW] Supports all Qwen 2.5 model sizes! 0.5, 1.5, 3, 7, 14, 32, 72b!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth --break-system-packages\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install transformers --break-system-packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install vllm --break-system-packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install -U ipywidgets --break-system-packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "ad4535433bec4b9da793d8d4d0c343b9",
            "928b8483557e4c24a094ae321934e603",
            "994d6a1841c04d16b2e23f857e02a2a2",
            "6b4a6a653cf847fdad67f00cc5bf40a1",
            "1ecb8e67633e426a8abeb7cb6b2a94b0",
            "fb302079688a4ec38f40db7f2152affc",
            "d0e81cea6fff446e900e9935102ba39b",
            "3e756bce896745208b546ff29bedc588",
            "38f7fca1681a43d0ae9b60ab1b32b494",
            "d9f96fa5f64340a8814e717117ee650e",
            "59a4a2de5d3b447398785efe68652070",
            "8761feab73ae4ae7a50ef8f475865a8a",
            "d7ac62b314934c3f966e0408d38ff40d",
            "fb1a46906bb647e7a7a34f6544f2e4c2",
            "bb8b4e11670347fab9c66d688a49efdc",
            "4e0647f0e19a4a749e5234fb9ad9ed6c",
            "a2fcad576d364606ad4b48d9a3e3d1e5",
            "2dfe4338fc8d49f7b8a7db1a679245db",
            "012b23379e76459097203a6e3c6da2d1",
            "1c56a9d43b1b49f2a629e13156a5bda3",
            "ae33466343294669aad52dce679a1e5a",
            "e6cc485feed5440e82c59d81a22eda1a",
            "95dcf5db38b948418ffac116978c79e0",
            "fef287bb485447a9ab924b27415162b0",
            "86e6f1d3d56644a4993215fc8b653a75",
            "233891f11da84da7baaa7d422722e25b",
            "9b4b15bee0b84d3cb06c441dfaf83e08",
            "405ef4fd2a1f4abeb7ea770df31cbe95",
            "e0cedc0b3a3b4fb5b51271e01d1b4ce1",
            "6206cdf8eb7d409daf4c354bf05d5a5a",
            "02296162d84240709420e1657fe844aa",
            "79224ecb4dd942238c3a1b0d7547fcfd",
            "f84900005b604655890ac85d58302470",
            "caf1e2598330493b83bc25d407254253",
            "b6a49f639e8f42e3a2729efe46095300",
            "57357203cbc04ad6bc2bb9d656ec427d",
            "98e72d58a62249019cffec5c448d8008",
            "d25b846aa2764ce4b124713fd9eff6b8",
            "e376aa61cdb843889f7b668861b5e79d",
            "d42f1b3c874d4b039540d540cf085bc9",
            "806acc65f38840c8bcd5d18e576ee43e",
            "b349e66fcf6b482fb25b00d088e598fd",
            "fecff859055446f1b7e9a59e52e65f4e",
            "0c31a70871f94005826d9713be86afce",
            "43d9011533f2429a8c778472ea210364",
            "ddf980e2b9584e578d902a653433cf45",
            "016f535b5932413ab7fae9f5146ea3cb",
            "662beba0fa5b4da6bfffcae33b7ccb17",
            "6bbc861ee3a44e94ac837138d118f1f0",
            "fd7d34a1f8a044b8aa1557d2995b75aa",
            "b8cedd30b50f418ba29870938086d1e3",
            "e4d05b08f4a9445c9ddd50992cce0938",
            "71dedeb9298c4066b336a9bdc6e7df24",
            "0aff71e75b3a4f6e949bca56a8df0100",
            "28c3a28934b7447eb66c7e40677a2145",
            "11275e276df241ba9e8ae12e5bbb5920",
            "4be44205996c421fb13aacc4f354c195",
            "79200b8c204b4a2491d971de6a7d5000",
            "4ebb39638b1f4f498ef84c2489dccbca",
            "f422ed73a308454189b9c78caa00dac2",
            "4010d72b00eb4a8b9544b239384ada3a",
            "55a82e68fa50480db6da1146c6041df2",
            "71a5bf2c803547ffb2b8b1f207cfd61c",
            "c04785ef763c41ab89dff3716c25ef3a",
            "409f332f7eeb455ebaa72e8695063a76",
            "356f7d525e1e461893d7cd804b2ad05e",
            "c4935f3afc6d46c1b1cb9fbbac2e3671",
            "25dd7b60b91f4253b6961a484b6f49e5",
            "f6804c70e72e4f7b9156ae940cc0ffb6",
            "24b2ab0f56be4ab6802fc5a5cd55ddcb",
            "2227250553684953b4356e450a745e25",
            "2dca35f0f36f4600ae1ad26f85ee334c",
            "1885b1b1d97c4a3fbda06848a0d26f18",
            "043da2a6d2ec4731abc62be73b1ec256",
            "2cfb07171af94af59aab313924474ebe",
            "869541e2ebb44b57bcac95ada442cf58",
            "4b8e4121779641dca57925a93fa394d9",
            "def592e7ad864884bce11edbb8c4b7f7",
            "2a4e9f051ac44c3caf1d5d9c3b058b85",
            "da3ca546480b450c86ab24d6fa9e2978",
            "def5e152032640d6b95e9f009df90963",
            "916d9ad909b04a059704955befc1af97",
            "4045b5b1f74840339603fe87f1216f12",
            "346a56af9c2f4dacb192376efa7be4f4",
            "c537633e14b84c30a08513b8470ac101",
            "eb5b1dcd47854716a0b00aacff5a3c59",
            "d833c00d7c3c4b6c98dee1b856a2efdf",
            "5017c4b2b1754f61b391e28901c35ad5"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "fff2d199-e331-4526-f29c-9b9bb87d86c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.2. vLLM: 0.8.3.\n",
            "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 4. Max memory: 47.413 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43dad61627ca4d009d609c39a3e30a1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # Can select any from the below:\n",
        "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
        "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
        "    # And also all Instruct versions and Math. Coding verisons!\n",
        "    model_name = \"unsloth/Qwen2.5-7B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "qwen2.5 gemma3 7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.51.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "33bb35b1-d4fc-4864-c992-6fc6ed6ee450"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token here (don't share it publicly!)\n",
        "login(\"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\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "3da513fa4fd04da59081f55559ca00bc",
            "aede3dad9b9d433097ac39bd55fdbd62",
            "b2732b52a0b74433a126789e7bd3e92d",
            "c0dbbeaa571e460580860cfdd171bc0a",
            "3099d76818e54823bc5134bc8dc3d0dc",
            "2a8cd87d40a045708ad1b33796407517",
            "c8e886d789de435c9bdc36f8565eeedb",
            "23f59923addb4171ad45b2566ee76cb9",
            "fad3b79e17e2406ca9ab74053c95d87f",
            "f4d35b1912be4bd0a7afe43ea23f6dea",
            "77803a040c514e159e4da54d607d7941",
            "9c8b3999ef754eeb8732160b1cae53fc",
            "2ee8a6712be340b19b59b7b7fdfcd98a",
            "ee11721b81d7494eb278fae5fccc67fb",
            "0a196c800cbe491fb1c7065bddd3c588",
            "3674772536b3464c99c02d38d7d9af03",
            "738117222691422f94577f1e302c4457",
            "457c28b6768e46b39d0e130bb1c08e15",
            "f05b6428c9364c978d294c597d9a5ba2",
            "e4958983acf44980a3736f620855b1d6",
            "4d663b64c47e49f49f7c263936e8758c",
            "efa54936e5a54297bd373a0251248698",
            "8112e8d3b5374226b9822a7f077f8172",
            "776df005bf804465ba34bf8dd7c97ef3",
            "87c90f1dc03b489588f1518aad5d1a7d",
            "accd3908a3224de3ac686877be939539",
            "b6fe6b42bbf7439db34366baf2e05931",
            "706555a787254ad0ae0fa7a855f6a695",
            "b8271d5f8f67474889dd76ac1ebdb51d",
            "8b99d74ffe5744d08b947fd7d2251bf3",
            "6b3c4f8bfa7d4777bc7e2742483ceac6",
            "99e0a76335df4c75b2aa571bbc5dc055",
            "62dc7f836a96477d952326cd5762564d",
            "54140ee830de4e1faf15caf0a64c5cbc",
            "6268fb00c3dc4f269803f687546c1176",
            "838facd69fa24b44995d6c0498945984",
            "e2cc75a947f7403bbf19932e5642113b",
            "c5f89151a2bb47bfab2fc8465fbb19cb",
            "20594405c23b4d38ac9b7da676ea8962",
            "f704dce299b24b32aab80f06b2cec4ad",
            "0179f9892e4b4a4eb335ee708e69d682",
            "b3c1bf9400aa4696ac93043c3585b6e5",
            "cd93921c00f74cd6b270dc5f4ac39949",
            "fe358767f7f548a3bb64fbdf881c3184"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "a024137b-87ef-41e6-9bb3-3217fedcab13"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"You are an expert assistant that translates natural language questions into Cypher queries for querying a Neo4j graph database. Use the provided schema to generate an accurate and syntactically correct Cypher query.\n",
        "\n",
        "### Schema:\n",
        "{}\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Cypher Query:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"schema\"]\n",
        "    inputs       = examples[\"question\"]\n",
        "    outputs      = examples[\"cypher\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset_1 = load_dataset(\"neo4j/text2cypher-2024v1\", split = \"train\")\n",
        "\n",
        "dataset = dataset_1.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['role_1', 'topic;', 'sub_topic', 'message_1', 'message_2'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Summarize this</td>\n",
              "      <td>This is a long paragraph...</td>\n",
              "      <td>A summary...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Translate to French</td>\n",
              "      <td>Hello, how are you?</td>\n",
              "      <td>Bonjour, comment ça va?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           instruction                        input                   output\n",
              "0       Summarize this  This is a long paragraph...             A summary...\n",
              "1  Translate to French          Hello, how are you?  Bonjour, comment ça va?"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"instruction\": [\"Summarize this\", \"Translate to French\"],\n",
        "    \"input\": [\"This is a long paragraph...\", \"Hello, how are you?\"],\n",
        "    \"output\": [\"A summary...\", \"Bonjour, comment ça va?\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output'],\n",
              "    num_rows: 2\n",
              "})"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "hf_dataset = Dataset.from_pandas(df)\n",
        "hf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "08f843a3245047f1951a8e4e26a4ca37",
            "96813422c0be495b88f85946436a3b59",
            "a422d3cf15874114a5fc797527cf1325",
            "94f1e363faea43b49af80a3929c04c37",
            "94599e4ca8234f62835e7513cf8fe002",
            "2260b74fb2d0436ab0c98a7d6e13b700",
            "2e586bccac5e44f2913a03141126e0f6",
            "f86b9a05a82c450184380a7ae625edbf",
            "4927a0d3f15b415daa59f5c10979c42c",
            "25f52d8c79684bf4b94c0196b1c77339",
            "c0f3ba7a3a0c4b91b1b0fd891e60a569"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "9493b067-d7d0-4ecb-9284-9db4e776cb81"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ba93c48ec044e4399437bdad269e1f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/20000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset= dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 2000,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "        save_steps = 200,                 # 👈 Optional but recommended\n",
        "        eval_steps = 200, \n",
        "        eval_strategy = \"steps\",   # 👈 Required\n",
        "        logging_strategy = \"steps\",         # 👈 Required\n",
        "        load_best_model_at_end=True,     # <- magic line!\n",
        "        metric_for_best_model=\"loss\", \n",
        "        greater_is_better = False, \n",
        "    ),\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "1e23a4e7-671a-4ca3-bab1-3a8f942abb48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA RTX A6000. Max memory = 47.413 GB.\n",
            "12.203 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "a2f43aee-1934-4d80-ff6c-b1c77c2294a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 20,000 | Num Epochs = 1 | Total steps = 200\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 8,798,208/5,000,000,000 (0.18% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='61' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 61/200 48:07 < 1:53:23, 0.02 it/s, Epoch 0.10/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.014900</td>\n",
              "      <td>1.025073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.029400</td>\n",
              "      <td>0.997813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.968600</td>\n",
              "      <td>0.983719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.016600</td>\n",
              "      <td>0.974524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.974700</td>\n",
              "      <td>0.967045</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1457' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1457/2500 05:03 < 03:37, 4.80 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/qwen2.5-0.5b-unsloth-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x786fba6cb890>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: bf5e4916-949b-45b2-a6b6-024510f15834)') - silently ignoring the lookup for the file config.json in unsloth/qwen2.5-0.5b-unsloth-bnb-4bit.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/qwen2.5-0.5b-unsloth-bnb-4bit - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/qwen2.5-0.5b-unsloth-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x786fba62e750>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: a01f87de-1336-4395-b899-995162d1e652)') - silently ignoring the lookup for the file config.json in unsloth/qwen2.5-0.5b-unsloth-bnb-4bit.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/qwen2.5-0.5b-unsloth-bnb-4bit - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.12/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/qwen2.5-0.5b-unsloth-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x786fdacefa40>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 39190dd8-ef00-444e-8c99-0357cfda29c2)') - silently ignoring the lookup for the file config.json in unsloth/qwen2.5-0.5b-unsloth-bnb-4bit.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/qwen2.5-0.5b-unsloth-bnb-4bit - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:381\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3096\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3094\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3096\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3097\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3045\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3048\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:4154\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4151\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4153\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4155\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4164\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:4348\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4345\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4347\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4348\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4349\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4350\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4352\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:4564\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4563\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4564\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4565\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m   4567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
            "File \u001b[0;32m~/Desktop/cadden/fine-tuning-models/unsloth_compiled_cache/UnslothSFTTrainer.py:748\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 748\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/unsloth/models/_utils.py:1029\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1029\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3800\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/unsloth/models/llama.py:1200\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1199\u001b[0m ):\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/unsloth/models/llama.py:1097\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m RETURN_LOGITS \u001b[38;5;129;01mand\u001b[39;00m HAS_CUT_CROSS_ENTROPY \u001b[38;5;129;01mand\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     n_items \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_items\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1097\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mfused_linear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1105\u001b[0m         output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/unsloth_zoo/loss_utils.py:170\u001b[0m, in \u001b[0;36mfused_linear_cross_entropy\u001b[0;34m(hidden_states, lm_weight, labels, num_items_in_batch, ignore_index, reduction, logit_softcapping, accuracy_threshold)\u001b[0m\n\u001b[1;32m    168\u001b[0m reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logit_softcapping \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: logit_softcapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_eps\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccuracy_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m num_items_in_batch\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/cut_cross_entropy/linear_cross_entropy.py:58\u001b[0m, in \u001b[0;36mlinear_cross_entropy\u001b[0;34m(e, c, targets, ignore_index, softcap, reduction, shift, filter_eps, impl)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCCE does not support MacOS. Please use torch_compile when running on MacOS instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cce_linear_cross_entropy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcce_linear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_eps\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_compile_linear_cross_entropy(\n\u001b[1;32m     63\u001b[0m         e, c, targets, ignore_index, softcap, reduction, shift\n\u001b[1;32m     64\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/cut_cross_entropy/cce.py:163\u001b[0m, in \u001b[0;36mcce_linear_cross_entropy\u001b[0;34m(e, c, targets, ignore_index, softcap, reduction, shift, filter_eps)\u001b[0m\n\u001b[1;32m    160\u001b[0m e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    161\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 163\u001b[0m valids \u001b[38;5;241m=\u001b[39m \u001b[43m_build_flat_valids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    166\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mflatten()\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/cut_cross_entropy/utils.py:38\u001b[0m, in \u001b[0;36m_build_flat_valids\u001b[0;34m(targets, ignore_index, shift)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 38\u001b[0m valids \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shift:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m valids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early Stopping\n",
        "\n",
        "class EarlyStopping:\n",
        "    def _init_(self, patience=5, verbose=False):\n",
        "      self.patience = patience\n",
        "      self.verbose = verbose\n",
        "      self.counter = 0\n",
        "      self.best_score =  None\n",
        "      self.early_stop = False\n",
        "      self.val_loss_min = float('inf')\n",
        "\n",
        "    def _call_(self, val_loss, model):\n",
        "      score = -val_loss\n",
        "      if self.best_score is None:\n",
        "        self.best_score = score\n",
        "        self.save_checkpoint(val_loss, model)\n",
        "      elif score < self.best_score:\n",
        "        self.counter += 1\n",
        "        if self.verbose:\n",
        "          print(f\"Early Stopping Counter: {self.counter} out of {self.patience}\")\n",
        "        if self.counter >= self.patience:\n",
        "          self.early_stop = True\n",
        "      else:\n",
        "        self.best_score = score\n",
        "        self.save_checkpoint(val_loss, model)\n",
        "        self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "            self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "# early_stopping = EarlyStopping()\n",
        "# early_stopping(average_val_loss, model)\n",
        "\n",
        "# if early_stopping.early_stop:\n",
        "#     print(\"Early stopping.\")\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "3005b362-2b86-468a-8f28-8effd9208566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3325.7276 seconds used for training.\n",
            "55.43 minutes used for training.\n",
            "Peak reserved memory = 2.551 GB.\n",
            "Peak reserved memory for training = 1.373 GB.\n",
            "Peak reserved memory % of max memory = 5.38 %.\n",
            "Peak reserved memory for training % of max memory = 2.896 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "4395a6c9-ee57-4f5f-84ba-a60fe5b915ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\nThe sequence of the Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones. The sequence begins with 1, 1, and continues as follows:\\n\\n1, 1, 2, 3, 5, 8, 13, 21,']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "187a7d22-bde7-439a-b1b0-a3e0dc308843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "The sequence of the Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones. The sequence begins with 1, 1, and continues as follows:\n",
            "\n",
            "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10\n"
          ]
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "0995f3eb-3eb9-423f-ede2-3b7f2f9f10c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 824.17 out of 1006.48 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:00<00:00, 182.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Done.\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at qwen_biology_gguf_model into bf16 GGUF format.\n",
            "The output location will be /home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.BF16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: qwen_biology_gguf_model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {896, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {896}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 896\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 4864\n",
            "INFO:hf-to-gguf:gguf: head count = 14\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151643\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.BF16.gguf: n_tensors = 290, total_size = 988.2M\n",
            "Writing: 100%|██████████| 988M/988M [00:02<00:00, 386Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 5162 (2016f07b)\n",
            "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
            "main: quantizing '/home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.BF16.gguf' to '/home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 224 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 290 tensors from /home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen_Biology_Gguf_Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 494M\n",
            "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 24\n",
            "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 896\n",
            "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 4864\n",
            "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 14\n",
            "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:  121 tensors\n",
            "llama_model_loader: - type bf16:  169 tensors\n",
            "[   1/ 290]                   output_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   2/ 290]                    token_embd.weight - [  896, 151936,     1,     1], type =   bf16, converting to q8_0 .. size =   259.66 MiB ->   137.94 MiB\n",
            "[   3/ 290]                    blk.0.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   4/ 290]                  blk.0.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[   5/ 290]               blk.0.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   6/ 290]             blk.0.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[   7/ 290]                    blk.0.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[   8/ 290]                  blk.0.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[   9/ 290]                    blk.0.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  10/ 290]                  blk.0.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[  11/ 290]                blk.0.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[  12/ 290]                blk.0.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  13/ 290]                blk.0.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  14/ 290]                  blk.0.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  15/ 290]                    blk.1.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  16/ 290]                  blk.1.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  17/ 290]               blk.1.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  18/ 290]             blk.1.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  19/ 290]                    blk.1.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  20/ 290]                  blk.1.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  21/ 290]                    blk.1.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  22/ 290]                  blk.1.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[  23/ 290]                blk.1.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[  24/ 290]                blk.1.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  25/ 290]                blk.1.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  26/ 290]                  blk.1.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  27/ 290]                    blk.2.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  28/ 290]                  blk.2.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  29/ 290]               blk.2.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  30/ 290]             blk.2.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  31/ 290]                    blk.2.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  32/ 290]                  blk.2.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  33/ 290]                    blk.2.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  34/ 290]                  blk.2.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[  35/ 290]                blk.2.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[  36/ 290]                blk.2.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  37/ 290]                blk.2.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  38/ 290]                  blk.2.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  39/ 290]                    blk.3.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  40/ 290]                  blk.3.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  41/ 290]               blk.3.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  42/ 290]             blk.3.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  43/ 290]                    blk.3.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  44/ 290]                  blk.3.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  45/ 290]                    blk.3.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  46/ 290]                  blk.3.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  47/ 290]                blk.3.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[  48/ 290]                blk.3.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  49/ 290]                blk.3.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  50/ 290]                  blk.3.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  51/ 290]                    blk.4.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  52/ 290]                  blk.4.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  53/ 290]               blk.4.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  54/ 290]             blk.4.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  55/ 290]                    blk.4.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  56/ 290]                  blk.4.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  57/ 290]                    blk.4.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  58/ 290]                  blk.4.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  59/ 290]                blk.4.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[  60/ 290]                blk.4.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  61/ 290]                blk.4.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  62/ 290]                  blk.4.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  63/ 290]                    blk.5.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  64/ 290]                  blk.5.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  65/ 290]               blk.5.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  66/ 290]             blk.5.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  67/ 290]                    blk.5.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  68/ 290]                  blk.5.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  69/ 290]                    blk.5.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  70/ 290]                  blk.5.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[  71/ 290]                blk.5.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[  72/ 290]                blk.5.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  73/ 290]                blk.5.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  74/ 290]                  blk.5.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  75/ 290]                    blk.6.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  76/ 290]                  blk.6.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  77/ 290]               blk.6.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  78/ 290]             blk.6.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  79/ 290]                    blk.6.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  80/ 290]                  blk.6.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  81/ 290]                    blk.6.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  82/ 290]                  blk.6.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  83/ 290]                blk.6.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[  84/ 290]                blk.6.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  85/ 290]                blk.6.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  86/ 290]                  blk.6.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  87/ 290]                    blk.7.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  88/ 290]                  blk.7.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  89/ 290]               blk.7.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  90/ 290]             blk.7.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  91/ 290]                    blk.7.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  92/ 290]                  blk.7.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[  93/ 290]                    blk.7.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  94/ 290]                  blk.7.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[  95/ 290]                blk.7.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[  96/ 290]                blk.7.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  97/ 290]                blk.7.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[  98/ 290]                  blk.7.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[  99/ 290]                    blk.8.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 100/ 290]                  blk.8.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 101/ 290]               blk.8.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 102/ 290]             blk.8.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 103/ 290]                    blk.8.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 104/ 290]                  blk.8.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 105/ 290]                    blk.8.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 106/ 290]                  blk.8.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 107/ 290]                blk.8.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 108/ 290]                blk.8.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 109/ 290]                blk.8.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 110/ 290]                  blk.8.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 111/ 290]                    blk.9.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 112/ 290]                  blk.9.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 113/ 290]               blk.9.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 114/ 290]             blk.9.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 115/ 290]                    blk.9.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 116/ 290]                  blk.9.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 117/ 290]                    blk.9.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 118/ 290]                  blk.9.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 119/ 290]                blk.9.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 120/ 290]                blk.9.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 121/ 290]                blk.9.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 122/ 290]                  blk.9.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 123/ 290]                   blk.10.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 124/ 290]                 blk.10.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 125/ 290]              blk.10.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 126/ 290]            blk.10.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 127/ 290]                   blk.10.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 128/ 290]                 blk.10.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 129/ 290]                   blk.10.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 130/ 290]                 blk.10.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 131/ 290]               blk.10.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 132/ 290]               blk.10.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 133/ 290]               blk.10.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 134/ 290]                 blk.10.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 135/ 290]                   blk.11.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 136/ 290]                 blk.11.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 137/ 290]              blk.11.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 138/ 290]            blk.11.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 139/ 290]                   blk.11.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 140/ 290]                 blk.11.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 141/ 290]                   blk.11.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 142/ 290]                 blk.11.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 143/ 290]               blk.11.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 144/ 290]               blk.11.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 145/ 290]               blk.11.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 146/ 290]                 blk.11.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 147/ 290]                   blk.12.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 148/ 290]                 blk.12.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 149/ 290]              blk.12.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 150/ 290]            blk.12.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 151/ 290]                   blk.12.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 152/ 290]                 blk.12.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 153/ 290]                   blk.12.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 154/ 290]                 blk.12.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 155/ 290]               blk.12.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 156/ 290]               blk.12.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 157/ 290]               blk.12.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 158/ 290]                 blk.12.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 159/ 290]                   blk.13.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 160/ 290]                 blk.13.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 161/ 290]              blk.13.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 162/ 290]            blk.13.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 163/ 290]                   blk.13.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 164/ 290]                 blk.13.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 165/ 290]                   blk.13.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 166/ 290]                 blk.13.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 167/ 290]               blk.13.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 168/ 290]               blk.13.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 169/ 290]               blk.13.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 170/ 290]                 blk.13.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 171/ 290]                   blk.14.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 172/ 290]                 blk.14.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 173/ 290]              blk.14.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 174/ 290]            blk.14.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 175/ 290]                   blk.14.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 176/ 290]                 blk.14.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 177/ 290]                   blk.14.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 178/ 290]                 blk.14.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 179/ 290]               blk.14.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 180/ 290]               blk.14.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 181/ 290]               blk.14.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 182/ 290]                 blk.14.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 183/ 290]                   blk.15.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 184/ 290]                 blk.15.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 185/ 290]              blk.15.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 186/ 290]            blk.15.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 187/ 290]                   blk.15.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 188/ 290]                 blk.15.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 189/ 290]                   blk.15.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 190/ 290]                 blk.15.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 191/ 290]               blk.15.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 192/ 290]               blk.15.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 193/ 290]               blk.15.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 194/ 290]                 blk.15.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 195/ 290]                   blk.16.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 196/ 290]                 blk.16.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 197/ 290]              blk.16.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 198/ 290]            blk.16.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 199/ 290]                   blk.16.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 200/ 290]                 blk.16.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 201/ 290]                   blk.16.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 202/ 290]                 blk.16.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 203/ 290]               blk.16.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 204/ 290]               blk.16.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 205/ 290]               blk.16.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 206/ 290]                 blk.16.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 207/ 290]                   blk.17.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 208/ 290]                 blk.17.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 209/ 290]              blk.17.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 210/ 290]            blk.17.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 211/ 290]                   blk.17.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 212/ 290]                 blk.17.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 213/ 290]                   blk.17.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 214/ 290]                 blk.17.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 215/ 290]               blk.17.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 216/ 290]               blk.17.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 217/ 290]               blk.17.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 218/ 290]                 blk.17.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 219/ 290]                   blk.18.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 220/ 290]                 blk.18.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 221/ 290]              blk.18.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 222/ 290]            blk.18.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 223/ 290]                   blk.18.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 224/ 290]                 blk.18.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 225/ 290]                   blk.18.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 226/ 290]                 blk.18.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 227/ 290]               blk.18.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 228/ 290]               blk.18.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 229/ 290]               blk.18.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 230/ 290]                 blk.18.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 231/ 290]                   blk.19.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 232/ 290]                 blk.19.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 233/ 290]              blk.19.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 234/ 290]            blk.19.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 235/ 290]                   blk.19.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 236/ 290]                 blk.19.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 237/ 290]                   blk.19.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 238/ 290]                 blk.19.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 239/ 290]               blk.19.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
            "[ 240/ 290]               blk.19.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 241/ 290]               blk.19.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 242/ 290]                 blk.19.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 243/ 290]                   blk.20.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 244/ 290]                 blk.20.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 245/ 290]              blk.20.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 246/ 290]            blk.20.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 247/ 290]                   blk.20.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 248/ 290]                 blk.20.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 249/ 290]                   blk.20.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 250/ 290]                 blk.20.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 251/ 290]               blk.20.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 252/ 290]               blk.20.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 253/ 290]               blk.20.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 254/ 290]                 blk.20.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 255/ 290]                   blk.21.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 256/ 290]                 blk.21.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 257/ 290]              blk.21.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 258/ 290]            blk.21.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 259/ 290]                   blk.21.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 260/ 290]                 blk.21.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 261/ 290]                   blk.21.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 262/ 290]                 blk.21.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 263/ 290]               blk.21.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 264/ 290]               blk.21.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 265/ 290]               blk.21.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 266/ 290]                 blk.21.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 267/ 290]                   blk.22.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 268/ 290]                 blk.22.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 269/ 290]              blk.22.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 270/ 290]            blk.22.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 271/ 290]                   blk.22.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 272/ 290]                 blk.22.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 273/ 290]                   blk.22.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 274/ 290]                 blk.22.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 275/ 290]               blk.22.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 276/ 290]               blk.22.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 277/ 290]               blk.22.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 278/ 290]                 blk.22.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 279/ 290]                   blk.23.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 280/ 290]                 blk.23.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
            "[ 281/ 290]              blk.23.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 282/ 290]            blk.23.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 283/ 290]                   blk.23.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 284/ 290]                 blk.23.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
            "[ 285/ 290]                   blk.23.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 286/ 290]                 blk.23.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
            "[ 287/ 290]               blk.23.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
            "[ 288/ 290]               blk.23.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "[ 289/ 290]               blk.23.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
            "[ 290/ 290]                 blk.23.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
            "llama_model_quantize_impl: model size  =   942.43 MB\n",
            "llama_model_quantize_impl: quant size  =   373.71 MB\n",
            "llama_model_quantize_impl: WARNING: 144 of 169 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time =  4165.22 ms\n",
            "main:    total time =  4165.23 ms\n",
            "Unsloth: Conversion completed! Output location: /home/ubuntu/Desktop/cadden/fine-tuning-models/qwen_biology_gguf_model/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "# model.save_pretrained(\"lora_model\") # Local saving\n",
        "model.save_pretrained_gguf(\"qwen_7b_cypher_gguf_model\", tokenizer, quantization_method = \"q4_k_m\") #gguf method :D\n",
        "\n",
        "# tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "0412171f-a9b4-4018-a4aa-d9725d2b8201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is a famous tall tower in Paris?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "One famous tall tower in Paris is the Eiffel Tower, also known as the Eiffel Tower or simply the Eiffel. It is a 324-meter-tall observation tower located in the 18th arrondissement of Paris, France. The tower was built in 1889 and is considered one of the most iconic landmarks in the city. It is located on the Champ de Mars, a square in the 18th arrondissement, and is a popular tourist attraction. The tower is a symbol of Paris and is a popular destination for visitors to the city.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "012b23379e76459097203a6e3c6da2d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "016f535b5932413ab7fae9f5146ea3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4d05b08f4a9445c9ddd50992cce0938",
            "max": 1671853,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71dedeb9298c4066b336a9bdc6e7df24",
            "value": 1671853
          }
        },
        "0179f9892e4b4a4eb335ee708e69d682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02296162d84240709420e1657fe844aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "043da2a6d2ec4731abc62be73b1ec256": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f843a3245047f1951a8e4e26a4ca37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96813422c0be495b88f85946436a3b59",
              "IPY_MODEL_a422d3cf15874114a5fc797527cf1325",
              "IPY_MODEL_94f1e363faea43b49af80a3929c04c37"
            ],
            "layout": "IPY_MODEL_94599e4ca8234f62835e7513cf8fe002"
          }
        },
        "0a196c800cbe491fb1c7065bddd3c588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d663b64c47e49f49f7c263936e8758c",
            "placeholder": "​",
            "style": "IPY_MODEL_efa54936e5a54297bd373a0251248698",
            "value": " 44.3M/44.3M [00:04&lt;00:00, 13.2MB/s]"
          }
        },
        "0aff71e75b3a4f6e949bca56a8df0100": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c31a70871f94005826d9713be86afce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11275e276df241ba9e8ae12e5bbb5920": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4be44205996c421fb13aacc4f354c195",
              "IPY_MODEL_79200b8c204b4a2491d971de6a7d5000",
              "IPY_MODEL_4ebb39638b1f4f498ef84c2489dccbca"
            ],
            "layout": "IPY_MODEL_f422ed73a308454189b9c78caa00dac2"
          }
        },
        "1885b1b1d97c4a3fbda06848a0d26f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c56a9d43b1b49f2a629e13156a5bda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ecb8e67633e426a8abeb7cb6b2a94b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20594405c23b4d38ac9b7da676ea8962": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2227250553684953b4356e450a745e25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2260b74fb2d0436ab0c98a7d6e13b700": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "233891f11da84da7baaa7d422722e25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79224ecb4dd942238c3a1b0d7547fcfd",
            "placeholder": "​",
            "style": "IPY_MODEL_f84900005b604655890ac85d58302470",
            "value": " 4.87k/4.87k [00:00&lt;00:00, 280kB/s]"
          }
        },
        "23f59923addb4171ad45b2566ee76cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b2ab0f56be4ab6802fc5a5cd55ddcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869541e2ebb44b57bcac95ada442cf58",
            "placeholder": "​",
            "style": "IPY_MODEL_4b8e4121779641dca57925a93fa394d9",
            "value": " 616/616 [00:00&lt;00:00, 46.0kB/s]"
          }
        },
        "25dd7b60b91f4253b6961a484b6f49e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dca35f0f36f4600ae1ad26f85ee334c",
            "placeholder": "​",
            "style": "IPY_MODEL_1885b1b1d97c4a3fbda06848a0d26f18",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "25f52d8c79684bf4b94c0196b1c77339": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c3a28934b7447eb66c7e40677a2145": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a4e9f051ac44c3caf1d5d9c3b058b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4045b5b1f74840339603fe87f1216f12",
            "placeholder": "​",
            "style": "IPY_MODEL_346a56af9c2f4dacb192376efa7be4f4",
            "value": "tokenizer.json: 100%"
          }
        },
        "2a8cd87d40a045708ad1b33796407517": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cfb07171af94af59aab313924474ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dca35f0f36f4600ae1ad26f85ee334c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dfe4338fc8d49f7b8a7db1a679245db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e586bccac5e44f2913a03141126e0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ee8a6712be340b19b59b7b7fdfcd98a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738117222691422f94577f1e302c4457",
            "placeholder": "​",
            "style": "IPY_MODEL_457c28b6768e46b39d0e130bb1c08e15",
            "value": "alpaca_data_cleaned.json: 100%"
          }
        },
        "3099d76818e54823bc5134bc8dc3d0dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346a56af9c2f4dacb192376efa7be4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "356f7d525e1e461893d7cd804b2ad05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3674772536b3464c99c02d38d7d9af03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f7fca1681a43d0ae9b60ab1b32b494": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3da513fa4fd04da59081f55559ca00bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aede3dad9b9d433097ac39bd55fdbd62",
              "IPY_MODEL_b2732b52a0b74433a126789e7bd3e92d",
              "IPY_MODEL_c0dbbeaa571e460580860cfdd171bc0a"
            ],
            "layout": "IPY_MODEL_3099d76818e54823bc5134bc8dc3d0dc"
          }
        },
        "3e756bce896745208b546ff29bedc588": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4010d72b00eb4a8b9544b239384ada3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4045b5b1f74840339603fe87f1216f12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "405ef4fd2a1f4abeb7ea770df31cbe95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "409f332f7eeb455ebaa72e8695063a76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43d9011533f2429a8c778472ea210364": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddf980e2b9584e578d902a653433cf45",
              "IPY_MODEL_016f535b5932413ab7fae9f5146ea3cb",
              "IPY_MODEL_662beba0fa5b4da6bfffcae33b7ccb17"
            ],
            "layout": "IPY_MODEL_6bbc861ee3a44e94ac837138d118f1f0"
          }
        },
        "457c28b6768e46b39d0e130bb1c08e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4927a0d3f15b415daa59f5c10979c42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b8e4121779641dca57925a93fa394d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4be44205996c421fb13aacc4f354c195": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4010d72b00eb4a8b9544b239384ada3a",
            "placeholder": "​",
            "style": "IPY_MODEL_55a82e68fa50480db6da1146c6041df2",
            "value": "added_tokens.json: 100%"
          }
        },
        "4d663b64c47e49f49f7c263936e8758c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e0647f0e19a4a749e5234fb9ad9ed6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ebb39638b1f4f498ef84c2489dccbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_409f332f7eeb455ebaa72e8695063a76",
            "placeholder": "​",
            "style": "IPY_MODEL_356f7d525e1e461893d7cd804b2ad05e",
            "value": " 632/632 [00:00&lt;00:00, 49.5kB/s]"
          }
        },
        "5017c4b2b1754f61b391e28901c35ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54140ee830de4e1faf15caf0a64c5cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6268fb00c3dc4f269803f687546c1176",
              "IPY_MODEL_838facd69fa24b44995d6c0498945984",
              "IPY_MODEL_e2cc75a947f7403bbf19932e5642113b"
            ],
            "layout": "IPY_MODEL_c5f89151a2bb47bfab2fc8465fbb19cb"
          }
        },
        "55a82e68fa50480db6da1146c6041df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57357203cbc04ad6bc2bb9d656ec427d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_806acc65f38840c8bcd5d18e576ee43e",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b349e66fcf6b482fb25b00d088e598fd",
            "value": 2776833
          }
        },
        "59a4a2de5d3b447398785efe68652070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6206cdf8eb7d409daf4c354bf05d5a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6268fb00c3dc4f269803f687546c1176": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20594405c23b4d38ac9b7da676ea8962",
            "placeholder": "​",
            "style": "IPY_MODEL_f704dce299b24b32aab80f06b2cec4ad",
            "value": "Map: 100%"
          }
        },
        "62dc7f836a96477d952326cd5762564d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "662beba0fa5b4da6bfffcae33b7ccb17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aff71e75b3a4f6e949bca56a8df0100",
            "placeholder": "​",
            "style": "IPY_MODEL_28c3a28934b7447eb66c7e40677a2145",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 24.0MB/s]"
          }
        },
        "6b3c4f8bfa7d4777bc7e2742483ceac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b4a6a653cf847fdad67f00cc5bf40a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9f96fa5f64340a8814e717117ee650e",
            "placeholder": "​",
            "style": "IPY_MODEL_59a4a2de5d3b447398785efe68652070",
            "value": " 5.55G/5.55G [00:54&lt;00:00, 262MB/s]"
          }
        },
        "6bbc861ee3a44e94ac837138d118f1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706555a787254ad0ae0fa7a855f6a695": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a5bf2c803547ffb2b8b1f207cfd61c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71dedeb9298c4066b336a9bdc6e7df24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "738117222691422f94577f1e302c4457": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776df005bf804465ba34bf8dd7c97ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_706555a787254ad0ae0fa7a855f6a695",
            "placeholder": "​",
            "style": "IPY_MODEL_b8271d5f8f67474889dd76ac1ebdb51d",
            "value": "Generating train split: 100%"
          }
        },
        "77803a040c514e159e4da54d607d7941": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79200b8c204b4a2491d971de6a7d5000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a5bf2c803547ffb2b8b1f207cfd61c",
            "max": 632,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c04785ef763c41ab89dff3716c25ef3a",
            "value": 632
          }
        },
        "79224ecb4dd942238c3a1b0d7547fcfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806acc65f38840c8bcd5d18e576ee43e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8112e8d3b5374226b9822a7f077f8172": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_776df005bf804465ba34bf8dd7c97ef3",
              "IPY_MODEL_87c90f1dc03b489588f1518aad5d1a7d",
              "IPY_MODEL_accd3908a3224de3ac686877be939539"
            ],
            "layout": "IPY_MODEL_b6fe6b42bbf7439db34366baf2e05931"
          }
        },
        "838facd69fa24b44995d6c0498945984": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0179f9892e4b4a4eb335ee708e69d682",
            "max": 51760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3c1bf9400aa4696ac93043c3585b6e5",
            "value": 51760
          }
        },
        "869541e2ebb44b57bcac95ada442cf58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e6f1d3d56644a4993215fc8b653a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6206cdf8eb7d409daf4c354bf05d5a5a",
            "max": 4865,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02296162d84240709420e1657fe844aa",
            "value": 4865
          }
        },
        "8761feab73ae4ae7a50ef8f475865a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7ac62b314934c3f966e0408d38ff40d",
              "IPY_MODEL_fb1a46906bb647e7a7a34f6544f2e4c2",
              "IPY_MODEL_bb8b4e11670347fab9c66d688a49efdc"
            ],
            "layout": "IPY_MODEL_4e0647f0e19a4a749e5234fb9ad9ed6c"
          }
        },
        "87c90f1dc03b489588f1518aad5d1a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b99d74ffe5744d08b947fd7d2251bf3",
            "max": 51760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b3c4f8bfa7d4777bc7e2742483ceac6",
            "value": 51760
          }
        },
        "8b99d74ffe5744d08b947fd7d2251bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "916d9ad909b04a059704955befc1af97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "928b8483557e4c24a094ae321934e603": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb302079688a4ec38f40db7f2152affc",
            "placeholder": "​",
            "style": "IPY_MODEL_d0e81cea6fff446e900e9935102ba39b",
            "value": "model.safetensors: 100%"
          }
        },
        "94599e4ca8234f62835e7513cf8fe002": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f1e363faea43b49af80a3929c04c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25f52d8c79684bf4b94c0196b1c77339",
            "placeholder": "​",
            "style": "IPY_MODEL_c0f3ba7a3a0c4b91b1b0fd891e60a569",
            "value": " 51760/51760 [00:49&lt;00:00, 1856.96 examples/s]"
          }
        },
        "95dcf5db38b948418ffac116978c79e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fef287bb485447a9ab924b27415162b0",
              "IPY_MODEL_86e6f1d3d56644a4993215fc8b653a75",
              "IPY_MODEL_233891f11da84da7baaa7d422722e25b"
            ],
            "layout": "IPY_MODEL_9b4b15bee0b84d3cb06c441dfaf83e08"
          }
        },
        "96813422c0be495b88f85946436a3b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2260b74fb2d0436ab0c98a7d6e13b700",
            "placeholder": "​",
            "style": "IPY_MODEL_2e586bccac5e44f2913a03141126e0f6",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "98e72d58a62249019cffec5c448d8008": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fecff859055446f1b7e9a59e52e65f4e",
            "placeholder": "​",
            "style": "IPY_MODEL_0c31a70871f94005826d9713be86afce",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 24.9MB/s]"
          }
        },
        "994d6a1841c04d16b2e23f857e02a2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e756bce896745208b546ff29bedc588",
            "max": 5547254637,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38f7fca1681a43d0ae9b60ab1b32b494",
            "value": 5547254108
          }
        },
        "99e0a76335df4c75b2aa571bbc5dc055": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4b15bee0b84d3cb06c441dfaf83e08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c8b3999ef754eeb8732160b1cae53fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ee8a6712be340b19b59b7b7fdfcd98a",
              "IPY_MODEL_ee11721b81d7494eb278fae5fccc67fb",
              "IPY_MODEL_0a196c800cbe491fb1c7065bddd3c588"
            ],
            "layout": "IPY_MODEL_3674772536b3464c99c02d38d7d9af03"
          }
        },
        "a2fcad576d364606ad4b48d9a3e3d1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a422d3cf15874114a5fc797527cf1325": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f86b9a05a82c450184380a7ae625edbf",
            "max": 51760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4927a0d3f15b415daa59f5c10979c42c",
            "value": 51760
          }
        },
        "accd3908a3224de3ac686877be939539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99e0a76335df4c75b2aa571bbc5dc055",
            "placeholder": "​",
            "style": "IPY_MODEL_62dc7f836a96477d952326cd5762564d",
            "value": " 51760/51760 [00:00&lt;00:00, 57055.45 examples/s]"
          }
        },
        "ad4535433bec4b9da793d8d4d0c343b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_928b8483557e4c24a094ae321934e603",
              "IPY_MODEL_994d6a1841c04d16b2e23f857e02a2a2",
              "IPY_MODEL_6b4a6a653cf847fdad67f00cc5bf40a1"
            ],
            "layout": "IPY_MODEL_1ecb8e67633e426a8abeb7cb6b2a94b0"
          }
        },
        "ae33466343294669aad52dce679a1e5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aede3dad9b9d433097ac39bd55fdbd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a8cd87d40a045708ad1b33796407517",
            "placeholder": "​",
            "style": "IPY_MODEL_c8e886d789de435c9bdc36f8565eeedb",
            "value": "README.md: 100%"
          }
        },
        "b2732b52a0b74433a126789e7bd3e92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23f59923addb4171ad45b2566ee76cb9",
            "max": 11610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fad3b79e17e2406ca9ab74053c95d87f",
            "value": 11610
          }
        },
        "b349e66fcf6b482fb25b00d088e598fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3c1bf9400aa4696ac93043c3585b6e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6a49f639e8f42e3a2729efe46095300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e376aa61cdb843889f7b668861b5e79d",
            "placeholder": "​",
            "style": "IPY_MODEL_d42f1b3c874d4b039540d540cf085bc9",
            "value": "vocab.json: 100%"
          }
        },
        "b6fe6b42bbf7439db34366baf2e05931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8271d5f8f67474889dd76ac1ebdb51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8cedd30b50f418ba29870938086d1e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb8b4e11670347fab9c66d688a49efdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae33466343294669aad52dce679a1e5a",
            "placeholder": "​",
            "style": "IPY_MODEL_e6cc485feed5440e82c59d81a22eda1a",
            "value": " 117/117 [00:00&lt;00:00, 7.05kB/s]"
          }
        },
        "c04785ef763c41ab89dff3716c25ef3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0dbbeaa571e460580860cfdd171bc0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4d35b1912be4bd0a7afe43ea23f6dea",
            "placeholder": "​",
            "style": "IPY_MODEL_77803a040c514e159e4da54d607d7941",
            "value": " 11.6k/11.6k [00:00&lt;00:00, 251kB/s]"
          }
        },
        "c0f3ba7a3a0c4b91b1b0fd891e60a569": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4935f3afc6d46c1b1cb9fbbac2e3671": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25dd7b60b91f4253b6961a484b6f49e5",
              "IPY_MODEL_f6804c70e72e4f7b9156ae940cc0ffb6",
              "IPY_MODEL_24b2ab0f56be4ab6802fc5a5cd55ddcb"
            ],
            "layout": "IPY_MODEL_2227250553684953b4356e450a745e25"
          }
        },
        "c537633e14b84c30a08513b8470ac101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f89151a2bb47bfab2fc8465fbb19cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e886d789de435c9bdc36f8565eeedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caf1e2598330493b83bc25d407254253": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6a49f639e8f42e3a2729efe46095300",
              "IPY_MODEL_57357203cbc04ad6bc2bb9d656ec427d",
              "IPY_MODEL_98e72d58a62249019cffec5c448d8008"
            ],
            "layout": "IPY_MODEL_d25b846aa2764ce4b124713fd9eff6b8"
          }
        },
        "cd93921c00f74cd6b270dc5f4ac39949": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e81cea6fff446e900e9935102ba39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d25b846aa2764ce4b124713fd9eff6b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d42f1b3c874d4b039540d540cf085bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7ac62b314934c3f966e0408d38ff40d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2fcad576d364606ad4b48d9a3e3d1e5",
            "placeholder": "​",
            "style": "IPY_MODEL_2dfe4338fc8d49f7b8a7db1a679245db",
            "value": "generation_config.json: 100%"
          }
        },
        "d833c00d7c3c4b6c98dee1b856a2efdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f96fa5f64340a8814e717117ee650e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da3ca546480b450c86ab24d6fa9e2978": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c537633e14b84c30a08513b8470ac101",
            "max": 7031863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb5b1dcd47854716a0b00aacff5a3c59",
            "value": 7031863
          }
        },
        "ddf980e2b9584e578d902a653433cf45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd7d34a1f8a044b8aa1557d2995b75aa",
            "placeholder": "​",
            "style": "IPY_MODEL_b8cedd30b50f418ba29870938086d1e3",
            "value": "merges.txt: 100%"
          }
        },
        "def592e7ad864884bce11edbb8c4b7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a4e9f051ac44c3caf1d5d9c3b058b85",
              "IPY_MODEL_da3ca546480b450c86ab24d6fa9e2978",
              "IPY_MODEL_def5e152032640d6b95e9f009df90963"
            ],
            "layout": "IPY_MODEL_916d9ad909b04a059704955befc1af97"
          }
        },
        "def5e152032640d6b95e9f009df90963": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d833c00d7c3c4b6c98dee1b856a2efdf",
            "placeholder": "​",
            "style": "IPY_MODEL_5017c4b2b1754f61b391e28901c35ad5",
            "value": " 7.03M/7.03M [00:00&lt;00:00, 35.8MB/s]"
          }
        },
        "e0cedc0b3a3b4fb5b51271e01d1b4ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2cc75a947f7403bbf19932e5642113b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd93921c00f74cd6b270dc5f4ac39949",
            "placeholder": "​",
            "style": "IPY_MODEL_fe358767f7f548a3bb64fbdf881c3184",
            "value": " 51760/51760 [00:01&lt;00:00, 56340.60 examples/s]"
          }
        },
        "e376aa61cdb843889f7b668861b5e79d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4958983acf44980a3736f620855b1d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4d05b08f4a9445c9ddd50992cce0938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6cc485feed5440e82c59d81a22eda1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb5b1dcd47854716a0b00aacff5a3c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee11721b81d7494eb278fae5fccc67fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f05b6428c9364c978d294c597d9a5ba2",
            "max": 44307561,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4958983acf44980a3736f620855b1d6",
            "value": 44307561
          }
        },
        "efa54936e5a54297bd373a0251248698": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f05b6428c9364c978d294c597d9a5ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f422ed73a308454189b9c78caa00dac2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4d35b1912be4bd0a7afe43ea23f6dea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6804c70e72e4f7b9156ae940cc0ffb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_043da2a6d2ec4731abc62be73b1ec256",
            "max": 616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cfb07171af94af59aab313924474ebe",
            "value": 616
          }
        },
        "f704dce299b24b32aab80f06b2cec4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f84900005b604655890ac85d58302470": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f86b9a05a82c450184380a7ae625edbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad3b79e17e2406ca9ab74053c95d87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb1a46906bb647e7a7a34f6544f2e4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_012b23379e76459097203a6e3c6da2d1",
            "max": 117,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c56a9d43b1b49f2a629e13156a5bda3",
            "value": 117
          }
        },
        "fb302079688a4ec38f40db7f2152affc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7d34a1f8a044b8aa1557d2995b75aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe358767f7f548a3bb64fbdf881c3184": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fecff859055446f1b7e9a59e52e65f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef287bb485447a9ab924b27415162b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_405ef4fd2a1f4abeb7ea770df31cbe95",
            "placeholder": "​",
            "style": "IPY_MODEL_e0cedc0b3a3b4fb5b51271e01d1b4ce1",
            "value": "tokenizer_config.json: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
